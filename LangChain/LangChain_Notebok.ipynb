{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LangChain Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "Key Components:\n",
    "* Chat Models: Unified way to use different LLMs from different providers\n",
    "* Prompt Templates: Unique approach to prompt management to allow injection of conversation history, reuse of prompts\n",
    "* Vector Stores: Integration package to convert text to embeddings for efficient retrieval of data for RAG applications\n",
    "* Document Loaders: Load in multiple documents (e.g. PDF, DOCX, HTML, Markdown, Powerpoint) from different resources to give knowledge to your chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dependencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the LangChain framework to build LLM powered applications, we will need to install these dependencies:\n",
    "* `langchain`: Contains the base infrastructure and some commonly used chains for RAG use cases.\n",
    "* `langchain_core`: Contains most of the abstractions which is key for building LLM applications. (e.g. Prompt Templates, Messages, Tools)\n",
    "* `langchain_openai`: Contains the OpenAI integration package which gives us access to OpenAI models\n",
    "* `langchain_community`: Contains the community integrations with different providers to load in documents (Document Loader) and index document (vector databases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain langchain_openai langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LangChain Basics - Prompting with Chat Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Messages Abstraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use the OpenAI SDK, we pass in our messages to the model in this manner:\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    ...,\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': '...'}.\n",
    "        {'role': 'user', 'content': '...'}.\n",
    "        {'role': 'assistant', 'content': '...'}.\n",
    "    ]\n",
    "    ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we were to use another providers which used token based prompting(e.g. Llama):\n",
    "```\n",
    "messages = <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful AI assistant for travel tips and recommendations<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What can you help me with?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain will unify the way we communicate with this models with the [`messages`](https://python.langchain.com/docs/concepts/messages/) abstraction:\n",
    "* `SystemMessage`: Corresponds to the `system` level prompt in all the different providers\n",
    "* `HumanMessage`: Corresponds to the `user` queries to the model\n",
    "* `AIMessage`: Corresponds to the `assistant` queries to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are an AI Chatbot who will reply to all responses in Singlish', additional_kwargs={}, response_metadata={}), HumanMessage(content='Complain about inflaton in Singapore', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "messages = [\n",
    "    SystemMessage(\"You are an AI Chatbot who will reply to all responses in Singlish\"),\n",
    "    HumanMessage(\"Complain about inflaton in Singapore\"),\n",
    "]\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Chat Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the messages we have defined above, we are ready to prompt for LLMs.\n",
    "\n",
    "We will import `ChatOpenAI` class from `langchain_openai` in order to use the enhanced functionalities that LangChain offers in their Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "client = ChatOpenAI(model = 'gpt-4o-mini', temperature = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the LLM response, we just need to use the `.invoke()` method which exists for most of the components in LangChain.\n",
    "\n",
    "**`.invoke()` is a standardised method across most of the LangChain components which takes in an input in a specified format and returns a specified output**\n",
    "\n",
    "Read what `.invoke()` does for the different chat models [here](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Wah, inflation in Singapore really quite jialat leh! Everything also so expensive now, like food, transport, even kopi also kena increase price. Last time can buy chicken rice for $3.50, now must pay $5 or more, siao ah! \\n\\nAnd don’t talk about housing, rental prices also sky high, many people struggling to find affordable place to stay. Sometimes feel like salary no increase but everything else keep going up, really stress one. Hope government can do something about it, otherwise how to survive like that?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 111, 'prompt_tokens': 33, 'total_tokens': 144, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None} id='run-cd7372d8-c7da-486f-8631-1d1c9a31b4b5-0' usage_metadata={'input_tokens': 33, 'output_tokens': 111, 'total_tokens': 144, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Wah, inflation in Singapore really quite jialat leh! Everything also so expensive now, like food, transport, even kopi also kena increase price. Last time can buy chicken rice for $3.50, now must pay $5 or more, siao ah! \n",
      "\n",
      "And don’t talk about housing, rental prices also sky high, many people struggling to find affordable place to stay. Sometimes feel like salary no increase but everything else keep going up, really stress one. Hope government can do something about it, otherwise how to survive like that?\n"
     ]
    }
   ],
   "source": [
    "response = client.invoke(messages)\n",
    "print(response)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LangChain Basics - Prompt Templates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prompt: Text Input that we pass into a chat model*\n",
    "\n",
    "*Prompt Template: Text Input that can be modified at run time that we pass into a chat model*\n",
    "\n",
    "With Prompt Templates, we can:\n",
    "* Reuse the same backbone template for similar task\n",
    "* Inject some arbitary information into the prompt for the LLM to produce more contextualised answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Translation Task with Prompt Templates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a template with different roles for the chat models, we first instantiate a `ChatPromptTemplate` and pass in a list of tuples in this syntax:\n",
    "`('role', 'instructions')`\n",
    "\n",
    "* `role` defines the type of message\n",
    "* `instructions/prompt` is the content of message corresponding to the role\n",
    "\n",
    "Any dynamic parts of the prompts that needs to be modified in runtime should be wrapped in `{...}`.\n",
    "(e.g. `('human', 'Translate {word} to English)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are an expert in translation from English to Chinese', additional_kwargs={}, response_metadata={}), HumanMessage(content='Translate 人工智能 to English', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = ChatPromptTemplate(\n",
    "    [\n",
    "        ('system', 'You are an expert in translation from English to Chinese'),\n",
    "        ('human', 'Translate {word} to English')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the template, we call the `.invoke()` method, providing a dictionary where the keys match the dynamic placeholders in the template and the values specify the content to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are an expert in translation from English to Chinese', additional_kwargs={}, response_metadata={}), HumanMessage(content='Translate 人工智能 to English', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(template.invoke(\n",
    "    {\n",
    "        'word': '人工智能',\n",
    "    }\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we get back a list of messages which is perfect to pass a chat model to give us a response:\n",
    "\n",
    "We can now translate different chinese words without typing the `\"Translate ... to English!\"` repeatedly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of \"人工智能\" to English is \"artificial intelligence.\"\n",
      "==============================\n",
      "The translation of \"神经网络\" to English is \"neural network.\"\n"
     ]
    }
   ],
   "source": [
    "prompt1 = template.invoke({\n",
    "    'word': '人工智能'\n",
    "})\n",
    "prompt2 = template.invoke({\n",
    "    'word': '神经网络'\n",
    "})\n",
    "print(client.invoke(prompt1).content)\n",
    "print('===' * 10)\n",
    "print(client.invoke(prompt2).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LangChain Bridging - Pydantic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pydantic is the most widely used data validation library in Python.\n",
    "\n",
    "With Pydantic's BaseModel, we can define a schema (ruleset) which will be then passed to the chat model as instructions to format our desired output.\n",
    "\n",
    "Pydantic will validate and coerce data, it will also catch for unexpected mistakes (e.g. LLM returns \"11\" instead of 11).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will try to model some attributes of a person (e.g. name, height).\n",
    "\n",
    "We first define a class called `Person` inheriting the propeties and methods from `BaseModel`.\n",
    "\n",
    "After so, we simply define the key name and its corresponding datatype in this format:\n",
    "`key: datatype`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person\"\"\"\n",
    "    name: str\n",
    "    height: float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our created model by passing in some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice\n",
      "1.8\n"
     ]
    }
   ],
   "source": [
    "person1 = Person(name = \"Alice\", height = 1.8)\n",
    "print(person1.name)\n",
    "print(person1.height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing in data with the wrong type will result in **validation error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person2 = Person(name = 000, height = \"abc\")\n",
    "print(person2.name)\n",
    "print(person2.height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Pydantic will also try to coerce some types if possible (e.g. `string` to `float`)\n",
    "\n",
    "In this example, we do not get any validation error when passing `height = \"1.72\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "person3 = Person(name = \"Alice\", height = \"1.72\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice\n",
      "1.72\n"
     ]
    }
   ],
   "source": [
    "print(person3.name)\n",
    "print(person3.height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that our `BaseModel` we create works optimally with chat models, we need to provide context for the different fields (e.g. what should be stored in that key)\n",
    "\n",
    "We will use the `Field` to acheive this, and in building LLM applications, this will act as instructions for the LLM to understand what information to populate into each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': FieldInfo(annotation=str, required=True, description='Name of the Person'), 'height': FieldInfo(annotation=float, required=True, description='Height of the person in meters')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4694/3237870130.py:7: PydanticDeprecatedSince20: The `__fields__` attribute is deprecated, use `model_fields` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  print(Person.__fields__)\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person\"\"\"\n",
    "    name: str = Field(description=\"Name of the Person\")\n",
    "    height: float = Field(description= \"Height of the person in meters\")\n",
    "print(Person.__fields__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LangChain Basics - Structured Output**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisite: Static Typing**\n",
    "\n",
    "**Prerequisite: Pydantic**  \n",
    "\n",
    "*If you're unfamiliar with Pydantic, be sure to watch the bridging lecture for an overview.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Information Extraction Task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this paragraph, extract the marks for the different students.\n",
    "\n",
    "```\n",
    "Adam scored 30 / 50 marks for math and 20 / 50 marks for science.\n",
    "Betty scored 1.5 times higher than Adam for math and just passed the science examination\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a Pydantic model which contains:\n",
    "* Fields as the different subject\n",
    "* Suitable Datatype for the different field\n",
    "* Description of the field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "class StudentMarks(BaseModel):\n",
    "    name: str = Field(..., description = \"Name of the student\")\n",
    "    math: float = Field(..., description=\"Student's Mark for Math\")\n",
    "    science: float = Field(..., description=\"Student's Mark for Science\")\n",
    "class ClassMarks(BaseModel):\n",
    "    marks: List[StudentMarks] = Field(..., description=\"Combined list of student marks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get structured output, we just need to bind our chat model with this `BaseModel` with the `.with_structured_output()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a normal chat model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model = 'gpt-4o-mini', temperature = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a structured output chat model with the `.with_structured_output()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_structured = llm.with_structured_output(ClassMarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke the chat model with structured output with the paragraph above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marks=[StudentMarks(name='Adam', math=30.0, science=20.0), StudentMarks(name='Betty', math=45.0, science=25.0)]\n"
     ]
    }
   ],
   "source": [
    "print(llm_structured.invoke([\"\"\"\n",
    "Adam scored 30 / 50 marks for math and 20 / 50 marks for science.\n",
    "Betty scored 1.5 times higher than Adam for math and just passed the science examination\n",
    "\"\"\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
